{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n",
      "\u001b[K     |████████████████████████████████| 769 kB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (3.0.12)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2020.7.14-cp36-cp36m-manylinux2010_x86_64.whl (660 kB)\n",
      "\u001b[K     |████████████████████████████████| 660 kB 16.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
      "  Downloading sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 12.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (1.18.1)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "\u001b[K     |████████████████████████████████| 883 kB 25.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.6)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (20.3)\n",
      "Collecting tokenizers==0.8.1.rc1\n",
      "  Downloading tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 32.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (4.44.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.14.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.1)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2020.4.5.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging->transformers) (2.4.6)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=a3e53a7010b038898e2c3ad0dc31c25f0d5f0cd722549f945fdf4a19bc4a349e\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: regex, sentencepiece, sacremoses, tokenizers, transformers\n",
      "Successfully installed regex-2020.7.14 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/modeling_auto.py:798: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=21128, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertConfig\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForMaskedLM\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "import torch\n",
    "\n",
    "model = 'bert-base-chinese'\n",
    "config = BertConfig.from_pretrained(model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "NUM_RESULTS = 3  \n",
    "model = AutoModelWithLMHead.from_pretrained(model)\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wwm BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = 'hfl/chinese-bert-wwm-ext'\n",
    "config = BertConfig.from_pretrained(model2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model2)\n",
    "\n",
    "NUM_RESULTS = 10  \n",
    "model2 = AutoModelWithLMHead.from_pretrained(model2)\n",
    "model2.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "def predict(text, model, num_results):\n",
    "    if isinstance(text, str):\n",
    "        tokenized_text = tokenizer.tokenize(text)\n",
    "    else:\n",
    "        tokenized_text = text \n",
    "    mask_ix = tokenized_text.index('[MASK]') \n",
    "    input_ids = torch.tensor(tokenizer.convert_tokens_to_ids(tokenized_text)).unsqueeze(0)\n",
    "    mask_distribution = model(input_ids)[0][:, mask_ix]\n",
    "    topk_probs, topk_vals = mask_distribution.topk(num_results)\n",
    "    return topk_probs, topk_vals, tokenized_text \n",
    "\n",
    "def predict_masks(text, choices, masks_left, num_results, model=model):\n",
    "    if masks_left < 1:\n",
    "        return \n",
    "    topk_probs, topk_vals, tokenized_text = predict(text, model, num_results)\n",
    "    if masks_left == 1:\n",
    "        for i in range(num_results):\n",
    "            token_id = int(topk_vals[0][i])\n",
    "            prob = float(topk_probs[0][i])\n",
    "            token = tokenizer._convert_id_to_token(token_id)\n",
    "            print(token, prob)\n",
    "        print() \n",
    "        return \n",
    "    for j in range(num_results):\n",
    "        token_id = int(topk_vals[0][j])\n",
    "        prob = float(topk_probs[0][j])\n",
    "        token = tokenizer._convert_id_to_token(token_id)\n",
    "        updated_text = copy.deepcopy(tokenized_text)\n",
    "        print(token, prob)\n",
    "        updated_text[updated_text.index('[MASK]')] = token \n",
    "        print(updated_text)\n",
    "        predict_masks(updated_text, choices, masks_left-1, num_results, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自 8.543882369995117\n",
      "['黑', '夜', '给', '了', '我', '黑', '色', '的', '眼', '睛', '我', '却', '用', '它', '寻', '找', '自', '[MASK]', '。']\n",
      "己 15.226829528808594\n",
      "由 12.742009162902832\n",
      "我 12.25773811340332\n",
      "\n",
      "生 8.116238594055176\n",
      "['黑', '夜', '给', '了', '我', '黑', '色', '的', '眼', '睛', '我', '却', '用', '它', '寻', '找', '生', '[MASK]', '。']\n",
      "活 13.305920600891113\n",
      "命 12.86294937133789\n",
      "存 10.92110538482666\n",
      "\n",
      "一 7.781883716583252\n",
      "['黑', '夜', '给', '了', '我', '黑', '色', '的', '眼', '睛', '我', '却', '用', '它', '寻', '找', '一', '[MASK]', '。']\n",
      "切 11.58308219909668\n",
      "生 10.99929141998291\n",
      "年 9.165112495422363\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "时 8.857612609863281\n",
      "['黑', '夜', '给', '了', '我', '黑', '色', '的', '眼', '睛', '我', '却', '用', '它', '寻', '找', '时', '[MASK]', '。']\n",
      "刻 18.439910888671875\n",
      "间 17.01576805114746\n",
      "代 14.826703071594238\n",
      "\n",
      "爱 8.689260482788086\n",
      "['黑', '夜', '给', '了', '我', '黑', '色', '的', '眼', '睛', '我', '却', '用', '它', '寻', '找', '爱', '[MASK]', '。']\n",
      "情 16.697908401489258\n",
      "间 12.10219955444336\n",
      "心 11.526802062988281\n",
      "\n",
      "生 8.664538383483887\n",
      "['黑', '夜', '给', '了', '我', '黑', '色', '的', '眼', '睛', '我', '却', '用', '它', '寻', '找', '生', '[MASK]', '。']\n",
      "活 18.877992630004883\n",
      "死 18.449121475219727\n",
      "命 17.331525802612305\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TEXT = \"\"\"\n",
    "黑夜给了我黑色的眼睛 我却用它寻找[MASK][MASK]。\n",
    "\"\"\"\n",
    "\n",
    "CHOICES = [] \n",
    "\n",
    "predict_masks(text=TEXT, choices=CHOICES, masks_left=2, num_results=3, model=model)\n",
    "print(\"------------------------------------------------------------------------------------\")\n",
    "predict_masks(text=TEXT, choices=CHOICES, masks_left=2, num_results=3, model=model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有 10.387682914733887\n",
      "['《', '成', '见', '》', '即', '使', '套', '上', '垃', '圾', '袋', '也', '不', '会', '有', '人', '觉', '得', '垃', '圾', '桶', '很', '有', '[MASK]']\n",
      "用 11.291385650634766\n",
      "名 10.417311668395996\n",
      "效 10.158729553222656\n",
      "\n",
      "重 9.647831916809082\n",
      "['《', '成', '见', '》', '即', '使', '套', '上', '垃', '圾', '袋', '也', '不', '会', '有', '人', '觉', '得', '垃', '圾', '桶', '很', '重', '[MASK]']\n",
      "的 8.205307006835938\n",
      "要 8.179563522338867\n",
      "大 8.116860389709473\n",
      "\n",
      "大 9.458673477172852\n",
      "['《', '成', '见', '》', '即', '使', '套', '上', '垃', '圾', '袋', '也', '不', '会', '有', '人', '觉', '得', '垃', '圾', '桶', '很', '大', '[MASK]']\n",
      "。 8.153440475463867\n",
      "的 7.610923767089844\n",
      "... 7.576132297515869\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "肮 10.73465633392334\n",
      "['《', '成', '见', '》', '即', '使', '套', '上', '垃', '圾', '袋', '也', '不', '会', '有', '人', '觉', '得', '垃', '圾', '桶', '很', '肮', '[MASK]']\n",
      "脏 20.972110748291016\n",
      "净 17.038986206054688\n",
      "肮 16.17559242248535\n",
      "\n",
      "脏 10.649730682373047\n",
      "['《', '成', '见', '》', '即', '使', '套', '上', '垃', '圾', '袋', '也', '不', '会', '有', '人', '觉', '得', '垃', '圾', '桶', '很', '脏', '[MASK]']\n",
      "而 9.532949447631836\n",
      ". 9.47905158996582\n",
      "所 9.333464622497559\n",
      "\n",
      "普 10.072741508483887\n",
      "['《', '成', '见', '》', '即', '使', '套', '上', '垃', '圾', '袋', '也', '不', '会', '有', '人', '觉', '得', '垃', '圾', '桶', '很', '普', '[MASK]']\n",
      "通 21.26113510131836\n",
      "遍 19.27455711364746\n",
      "普 17.53107452392578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TEXT = \"\"\"\n",
    "《成见》\n",
    "即使套上垃圾袋\n",
    "也不会有人\n",
    "觉得垃圾桶很[MASK][MASK]\n",
    "\"\"\"\n",
    "\n",
    "CHOICES = []\n",
    "predict_masks(text=TEXT, choices=CHOICES, masks_left=2, num_results=3, model=model)\n",
    "print(\"------------------------------------------------------------------------------------\")\n",
    "predict_masks(text=TEXT, choices=CHOICES, masks_left=2, num_results=3, model=model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "王 6.742694854736328\n",
      "['《', '说', '服', '》', '你', '是', '冰', '我', '是', '王', '[MASK]']\n",
      "者 7.674739837646484\n",
      "， 6.377195358276367\n",
      "子 6.348962783813477\n",
      "\n",
      "马 6.220652103424072\n",
      "['《', '说', '服', '》', '你', '是', '冰', '我', '是', '马', '[MASK]']\n",
      "（ 7.140013217926025\n",
      "儿 6.351709365844727\n",
      "， 6.261220455169678\n",
      "\n",
      "台 5.686470031738281\n",
      "['《', '说', '服', '》', '你', '是', '冰', '我', '是', '台', '[MASK]']\n",
      "湾 8.377700805664062\n",
      "马 6.711005210876465\n",
      "小 6.70683479309082\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "中 8.277717590332031\n",
      "['《', '说', '服', '》', '你', '是', '冰', '我', '是', '中', '[MASK]']\n",
      "国 10.422002792358398\n",
      "文 8.68859577178955\n",
      "华 8.126713752746582\n",
      "\n",
      "游 7.909826278686523\n",
      "['《', '说', '服', '》', '你', '是', '冰', '我', '是', '游', '[MASK]']\n",
      "戏 17.488126754760742\n",
      "戲 13.181442260742188\n",
      "乐 11.974376678466797\n",
      "\n",
      "热 7.883047580718994\n",
      "['《', '说', '服', '》', '你', '是', '冰', '我', '是', '热', '[MASK]']\n",
      "热 11.148796081542969\n",
      "儿 8.764961242675781\n",
      "/ 8.298961639404297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TEXT = \"\"\"\n",
    "《说服》\n",
    "你是冰\n",
    "我是[MASK][MASK]\n",
    "\"\"\"\n",
    "\n",
    "CHOICES = []\n",
    "\n",
    "predict_masks(text=TEXT, choices=CHOICES, masks_left=2, num_results=3, model=model)\n",
    "print(\"------------------------------------------------------------------------------------\")\n",
    "predict_masks(text=TEXT, choices=CHOICES, masks_left=2, num_results=3, model=model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 6.648489952087402\n",
      "['《', '世', '界', '》', '我', '[MASK]', '不', '开', '了', '花', '继', '续', '开']\n",
      "们 11.359298706054688\n",
      "也 10.413335800170898\n",
      "还 9.616384506225586\n",
      "\n",
      "， 6.5934600830078125\n",
      "['《', '世', '界', '》', '，', '[MASK]', '不', '开', '了', '花', '继', '续', '开']\n",
      "爱 9.663169860839844\n",
      "也 8.148285865783691\n",
      "永 7.861174583435059\n",
      "\n",
      "《 6.3722076416015625\n",
      "['《', '世', '界', '》', '《', '[MASK]', '不', '开', '了', '花', '继', '续', '开']\n",
      "花 8.496639251708984\n",
      "爱 8.486017227172852\n",
      "新 7.992000102996826\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "大 7.0296831130981445\n",
      "['《', '世', '界', '》', '大', '[MASK]', '不', '开', '了', '花', '继', '续', '开']\n",
      "河 9.451949119567871\n",
      "山 9.290780067443848\n",
      "海 8.543252944946289\n",
      "\n",
      "一 6.896337509155273\n",
      "['《', '世', '界', '》', '一', '[MASK]', '不', '开', '了', '花', '继', '续', '开']\n",
      "天 10.683612823486328\n",
      "地 9.901617050170898\n",
      "口 9.817977905273438\n",
      "\n",
      "小 6.63884162902832\n",
      "['《', '世', '界', '》', '小', '[MASK]', '不', '开', '了', '花', '继', '续', '开']\n",
      "山 10.4845609664917\n",
      "谜 9.732901573181152\n",
      "诗 9.433244705200195\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TEXT = \"\"\"\n",
    "《世界》\n",
    "[MASK][MASK]不开了\n",
    "花继续开\n",
    "\"\"\"\n",
    "\n",
    "CHOICES = []\n",
    "\n",
    "predict_masks(text=TEXT, choices=CHOICES, masks_left=2, num_results=3, model=model)\n",
    "print(\"------------------------------------------------------------------------------------\")\n",
    "predict_masks(text=TEXT, choices=CHOICES, masks_left=2, num_results=3, model=model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "鲨 9.557595252990723\n",
      "['《', '安', '慰', '》', '罐', '头', '告', '诉', '鱼', '这', '里', '很', '安', '全', '没', '有', '鲨', '鱼', '也', '没', '有', '鲨', '[MASK]']\n",
      "鱼 15.059053421020508\n",
      "牙 7.787806987762451\n",
      "影 7.576664924621582\n",
      "\n",
      "鱼 9.086846351623535\n",
      "['《', '安', '慰', '》', '罐', '头', '告', '诉', '鱼', '这', '里', '很', '安', '全', '没', '有', '鲨', '鱼', '也', '没', '有', '鱼', '[MASK]']\n",
      "也 8.869028091430664\n",
      "鱼 8.161286354064941\n",
      "毒 8.094161033630371\n",
      "\n",
      "海 8.67890739440918\n",
      "['《', '安', '慰', '》', '罐', '头', '告', '诉', '鱼', '这', '里', '很', '安', '全', '没', '有', '鲨', '鱼', '也', '没', '有', '海', '[MASK]']\n",
      "豚 12.03481674194336\n",
      "底 10.261301040649414\n",
      "鱼 9.988921165466309\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "海 9.457998275756836\n",
      "['《', '安', '慰', '》', '罐', '头', '告', '诉', '鱼', '这', '里', '很', '安', '全', '没', '有', '鲨', '鱼', '也', '没', '有', '海', '[MASK]']\n",
      "洋 10.992998123168945\n",
      "底 10.177282333374023\n",
      "面 9.573942184448242\n",
      "\n",
      "大 9.067609786987305\n",
      "['《', '安', '慰', '》', '罐', '头', '告', '诉', '鱼', '这', '里', '很', '安', '全', '没', '有', '鲨', '鱼', '也', '没', '有', '大', '[MASK]']\n",
      "大 9.797916412353516\n",
      "的 9.441535949707031\n",
      "型 8.973787307739258\n",
      "\n",
      "鲨 8.83832836151123\n",
      "['《', '安', '慰', '》', '罐', '头', '告', '诉', '鱼', '这', '里', '很', '安', '全', '没', '有', '鲨', '鱼', '也', '没', '有', '鲨', '[MASK]']\n",
      "鱼 19.116743087768555\n",
      "魚 11.457351684570312\n",
      "水 10.754939079284668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TEXT = \"\"\"\n",
    "《安慰》\n",
    "罐头告诉鱼\n",
    "这里很安全\n",
    "没有鲨鱼\n",
    "也没有[MASK][MASK]\n",
    "\"\"\"\n",
    "\n",
    "CHOICES = []\n",
    "\n",
    "predict_masks(text=TEXT, choices=CHOICES, masks_left=2, num_results=3, model=model)\n",
    "print(\"------------------------------------------------------------------------------------\")\n",
    "predict_masks(text=TEXT, choices=CHOICES, masks_left=2, num_results=3, model=model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "它 12.4714937210083\n",
      "到 11.505626678466797\n",
      "看 10.755748748779297\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "它 12.4714937210083\n",
      "到 11.505626678466797\n",
      "看 10.755748748779297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TEXT = \"\"\"\n",
    "《甜甜圈》\n",
    "生活有时会出现一个大洞\n",
    "我们可以只看[MASK]的部分\n",
    "\"\"\"\n",
    "predict_masks(text=TEXT, choices=CHOICES, masks_left=1, num_results=3, model=model2)\n",
    "print(\"------------------------------------------------------------------------------------\")\n",
    "predict_masks(text=TEXT, choices=CHOICES, masks_left=1, num_results=3, model=model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "做 8.110145568847656\n",
      "用 7.727746486663818\n",
      "说 6.990058422088623\n",
      "代 6.6633453369140625\n",
      "马 6.506006717681885\n",
      "的 6.326287269592285\n",
      "学 6.135912895202637\n",
      "生 6.12570333480835\n",
      "方 6.1252946853637695\n",
      "使 6.069259166717529\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "说 11.365616798400879\n",
      "进 10.413785934448242\n",
      "以 10.25168228149414\n",
      "选 10.208749771118164\n",
      "生 10.142194747924805\n",
      "做 9.973831176757812\n",
      "活 9.749025344848633\n",
      "自 9.701735496520996\n",
      "出 9.658236503601074\n",
      "去 9.648157119750977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TEXT = \"\"\"\n",
    "《悲剧》\n",
    "公主被王子吻醒\n",
    "发现四周没有别人可以[MASK]\n",
    "\"\"\"\n",
    "\n",
    "CHOICES = []\n",
    "\n",
    "predict_masks(text=TEXT, choices=CHOICES, masks_left=1, num_results=NUM_RESULTS, model=model)\n",
    "print(\"------------------------------------------------------------------------------------\")\n",
    "predict_masks(text=TEXT, choices=CHOICES, masks_left=1, num_results=NUM_RESULTS, model=model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... 7.920332908630371\n",
      "['《', '挫', '败', '》', '像', '在', '大', '雨', '中', '被', '风', '吹', '坏', '一', '把', '伞', '有', '些', '人', '学', '会', '买', '坚', '固', '的', '伞', '有', '些', '人', '学', '会', '...', '[MASK]', '不', '出', '门']\n",
      "永 8.29880142211914\n",
      "三 7.894652843475342\n",
      "也 7.70860481262207\n",
      "\n",
      "老 7.33214807510376\n",
      "['《', '挫', '败', '》', '像', '在', '大', '雨', '中', '被', '风', '吹', '坏', '一', '把', '伞', '有', '些', '人', '学', '会', '买', '坚', '固', '的', '伞', '有', '些', '人', '学', '会', '老', '[MASK]', '不', '出', '门']\n",
      "年 11.802939414978027\n",
      "天 10.735246658325195\n",
      "子 10.58016586303711\n",
      "\n",
      "不 6.851112365722656\n",
      "['《', '挫', '败', '》', '像', '在', '大', '雨', '中', '被', '风', '吹', '坏', '一', '把', '伞', '有', '些', '人', '学', '会', '买', '坚', '固', '的', '伞', '有', '些', '人', '学', '会', '不', '[MASK]', '不', '出', '门']\n",
      "离 10.578926086425781\n",
      "辞 9.912263870239258\n",
      "理 9.82224178314209\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "工 9.829963684082031\n",
      "['《', '挫', '败', '》', '像', '在', '大', '雨', '中', '被', '风', '吹', '坏', '一', '把', '伞', '有', '些', '人', '学', '会', '买', '坚', '固', '的', '伞', '有', '些', '人', '学', '会', '工', '[MASK]', '不', '出', '门']\n",
      "作 18.65856170654297\n",
      "厂 14.587923049926758\n",
      "商 13.74268627166748\n",
      "\n",
      "今 9.373473167419434\n",
      "['《', '挫', '败', '》', '像', '在', '大', '雨', '中', '被', '风', '吹', '坏', '一', '把', '伞', '有', '些', '人', '学', '会', '买', '坚', '固', '的', '伞', '有', '些', '人', '学', '会', '今', '[MASK]', '不', '出', '门']\n",
      "天 19.21027374267578\n",
      "后 17.834047317504883\n",
      "年 17.585657119750977\n",
      "\n",
      "现 9.325822830200195\n",
      "['《', '挫', '败', '》', '像', '在', '大', '雨', '中', '被', '风', '吹', '坏', '一', '把', '伞', '有', '些', '人', '学', '会', '买', '坚', '固', '的', '伞', '有', '些', '人', '学', '会', '现', '[MASK]', '不', '出', '门']\n",
      "在 17.461532592773438\n",
      "实 15.241813659667969\n",
      "今 14.728378295898438\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TEXT = \"\"\"\n",
    "《挫败》\n",
    "像在大雨中被风吹坏一把伞\n",
    "有些人学会买坚固的伞\n",
    "有些人学会[MASK][MASK]不出门\n",
    "\"\"\"\n",
    "\n",
    "CHOICES = []\n",
    "\n",
    "predict_masks(text=TEXT, choices=CHOICES, masks_left=2, num_results=3, model=model)\n",
    "print(\"------------------------------------------------------------------------------------\")\n",
    "predict_masks(text=TEXT, choices=CHOICES, masks_left=2, num_results=3, model=model2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
